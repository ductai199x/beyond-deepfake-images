<!doctype html>
<html lang="en">
<script>
  window.onload = function () {
    if (window == window.parent) {
      document.body.innerHTML = '';
    }
  };  
</script>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
</head>

<body>
  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <span>
        <a target="_blank"
          href="https://scholar.google.com/citations?user=FzqABy8AAAAJ">Danial&nbsp;Samadi&nbsp;Vahdati</a>,
        <a target="_blank" href="https://scholar.google.com/citations?user=CRpUoW4AAAAJ">Tai&nbsp;D.&nbsp;Nguyen</a>,
        <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=5EDdOgkAAAAJ">Aref&nbsp;Azizpour</a>,
        <a target="_blank" href="https://scholar.google.com/citations?user=UE1rg1IAAAAJ">Matthew&nbsp;C.&nbsp;Stamm</a>
      </span>
      <span>
        Drexel University<br />Philadelphia, USA<br />
        { ds3729, tdn47, aa4639, mcs382 }@drexel.edu
      </span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <a target="_blank" href=""><button>arXiv (Pending) </button></a>
        <a target="_blank" href="./static/media/main.pdf"><button>PDF</button></a>
      </div>
      <small><span>
        <b>TL;DR</b>: Synthetic image detectors don't work on synthetically generated videos. We provide empirical evidence as to why and propose ways to adapt existing detectors to work on synthetic videos.
      </span></small>
      <div class='responsive-image-container'>
        <img src='image/teaser.jpg' alt='' />
        <small><p>Top row: video frames taken from AI-generated videos. Bottom row: Fourier transforms of the residual forensic traces for each corresponding frame above (what current detectors likely see).</p></small>
      </div>
    </div>
    <div id="abstract-flex" class="column-flex">
      <h2>Abstract</h2>
      <small>
        <p>
          Recent advances in generative AI have led to the development of techniques to generate visually realistic synthetic video. While a number of techniques have been developed to detect AI-generated synthetic images, in this paper we show that synthetic image detectors are unable to detect synthetic videos. We demonstrate that this is because synthetic video generators introduce substantially different traces than those left by image generators. Despite this, we show that synthetic video traces can be learned, and used to perform reliable synthetic video detection or generator source attribution even after H.264 re-compression. Furthermore, we demonstrate that while detecting videos from new generators through zero-shot transferability is challenging, accurate detection of videos from a new generator can be achieved through few-shot learning.
        </p>
      </small>
    </div>
    <div id="sections" class="column-flex">
      <!-- <div class='responsive-image-container'>
        <img src='image/synth_frames.jpg' alt='' />
      </div> -->
      <h3>Synthetic Image Detectors Don't Work On Videos</h3>
      <p>
        Given that a video can be seen as a sequence of images, it is reasonable to expect that synthetic image detectors should be effective at detecting AI-generated synthetic videos. Surprisingly, however, we have found that synthetic image detectors do not successfully identify synthetic videos. Furthermore, we have found that this issue is not primarily caused by the degradation of forensic traces due to H.264 video compression.
      </p>
      <div class='responsive-image-container row-flex'>
        <div><img src='image/result_1.jpg' alt='' /></div>
        <div><img src='image/result_2.jpg' alt='' /></div>
      </div>
      <small><p>Left: Detection performance of existing synthetic image generators on synthetic videos. Right: Similar to Left but these detectors were robustly trained on H.264 compressed synthetic images.</p></small>

      <h3>Synthetic Video Traces</h3>
      <p>
        Here, we present evidence that forensic traces in synthetic video are substantially different than those in synthetic images.  We qualitatively demonstrate this by visualizing the low-level forensic traces left by a number of different image and video generators. Due to the stark contrasts between forensic traces left by image and video generators, it is highly likely that this is the major reason why synthetic image detectors exhibit substantially lower performance on video.  Even when robustly trained, synthetic image detectors learn features to capture forensic traces similar to what they have seen before.  Since video traces can be substantially different in nature, synthetic image detectors are not suited to capture these traces.  
      </p>
      <div class='responsive-image-container'>
        <div><img src='image/trace_viz.jpg' alt='' /></div>
      </div>
      <small><p>Fourier transforms of the residual forensic traces for each frame of a synthetic video. The traces are visually distinct from those of synthetic images.</p></small>


      <h3>Learning Synthetic Video Traces</h3>
      <p>
        Results presented in the previous two sections show that traces left by synthetic video generators are different than those left by image generators, and that synthetic image detectors do not reliably detect these traces.
        In this section, however, we show that synthetic video traces can be learned.  Through a series of experiments, we show that CNNs can be trained to accurately perform synthetic video detection and source attribution.   
        Furthermore, we demonstrate that robust training can improve these detectors even after H.264 re-compression.  Additionally, we show how video-level detection can be performed to boost performance over frame-level detection.
      </p>
      <div class='responsive-image-container row-flex'>
        <div><img src='image/result_3.jpg' alt='' /></div>
        <div><img src='image/result_4.jpg' alt='' /></div>
      </div>
      <small><p>Left: Frame level detection performance of synthetic video detectors. Right: Frame level attribution performance of synthetic video detectors.</p></small>

      <div class='responsive-image-container'>
        <div><img src='image/result_5.jpg' alt='' /></div>
      </div>
      <small><p>Detection performance of the MISLnet architecture before and after robust-training on real & synthetic videos with Constant Rate Factor (CRF) of 40.</p></small>
      
      <div class='responsive-image-container row-flex'>
        <div><img src='image/result_6.jpg' alt='' /></div>
        <div><img src='image/result_7.jpg' alt='' /></div>
      </div>
      <small><p>Left: Video-level performance of MISLnet over different number of patches used for obtaining video-level detection score. Right: Relative Error Reduction in video-level performance versus frame-level performance of MISLnet over different number of patches used for obtaining video-level detection score.</p></small>

      <h3>Detection Transferability to New Generators</h3>
      <p>
        We examined synthetic video detectors' zero-shot transferability performance.  This corresponds to a detector's ability to detect videos from a new generator without any re-training. These results show that while the detector achieves strong performance on videos from generators seen during training, performance drops significantly when evaluating on new generators. This is unsurprising because traces left by different generators can vary substantially. As a result, it is  difficult for a detector to capture traces from new generation that leave different traces than those seen in training. 
      </p>
      <p>
        Additionally, we performed few-shot learning experiments to evaluate the detector's ability to detect videos from new generators with only a few examples. These results show that the detector can very accurately transfer to detect new generators through few-shot learning.  This is particularly important given the rapid pace with which new generators such as Sora are emerging.
      </p>
      <div class='responsive-image-container row-flex'>
        <div><img src='image/result_8.jpg' alt='' /></div>
        <div><img src='image/result_9.jpg' alt='' /></div>
      </div>
      <small><p>Left: Zero-shot detection performance of MISLnet, which was trained on 3 out of 4 synthetic video generation sources and test on the remaining one. Performance numbers are in AUC. Right: Zero-Shot and Few-Shot detection performance of MISLnet, which was trained on all training generators, and tested on new generation sources. Performance numbers are measured using AUC and RER.</p></small>
      
      <h3>Bibtex</h3>
      <pre>
        <code>
          @InProceedings{Vahdati_2024_CVPR,
              author    = {Vahdati, Danial Samadi and Nguyen, Tai D. and Azizpour, Aref and Stamm, Matthew C.},
              title     = {Beyond Deepfake Images: Detecting AI-Generated Videos},
              booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
              month     = {June},
              year      = {2024},
          }
        </code>
      </pre>

    </div>
  </div>
  <script src="index.js"></script>
</body>


</html>